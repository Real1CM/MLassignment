{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONvhLq3kuWZjZZtxvYAvOE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Real1CM/MLassignment/blob/main/%E4%BD%9C%E4%B8%9A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "只跑了10轮，卡带不动"
      ],
      "metadata": {
        "id": "SSkHpe_nUBlI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSM9E9cMSPKD",
        "outputId": "cbcf8e21-fad4-436d-91e2-75a34bccfdd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10, Total Reward: 83.0\n",
            "Episode 2/10, Total Reward: 13.0\n",
            "Episode 3/10, Total Reward: 9.0\n",
            "Episode 4/10, Total Reward: 11.0\n",
            "Episode 5/10, Total Reward: 22.0\n",
            "Episode 6/10, Total Reward: 9.0\n",
            "Episode 7/10, Total Reward: 8.0\n",
            "Episode 8/10, Total Reward: 9.0\n",
            "Episode 9/10, Total Reward: 14.0\n",
            "Episode 10/10, Total Reward: 13.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 创建CartPole环境\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Q值网络\n",
        "def build_q_network(state_size, action_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(24, activation='relu', input_shape=(state_size,)),\n",
        "        layers.Dense(24, activation='relu'),\n",
        "        layers.Dense(action_size, activation='linear')  # 输出Q值\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Q-learning超参数\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "gamma = 0.99  # 折扣因子\n",
        "epsilon = 0.1  # 探索率\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "memory = []  # 存储经验回放\n",
        "\n",
        "# 初始化Q网络和优化器\n",
        "model = build_q_network(state_size, action_size)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# 选择动作的函数（根据epsilon-greedy策略）\n",
        "def select_action(state):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return np.random.choice(action_size)  # 探索\n",
        "    q_values = model(state[np.newaxis])  # 使用模型预测Q值\n",
        "    return np.argmax(q_values)  # 利用\n",
        "\n",
        "# 存储经验的函数\n",
        "def store_experience(state, action, reward, next_state, done):\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "    if len(memory) > 10000:  # 限制经验回放的大小\n",
        "        memory.pop(0)\n",
        "\n",
        "# 从经验回放中采样并训练模型\n",
        "def train_q_network():\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    batch = np.random.choice(len(memory), batch_size, replace=False)\n",
        "    for idx in batch:\n",
        "        state, action, reward, next_state, done = memory[idx]\n",
        "        state = np.array(state, dtype=np.float32)\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = model(state[np.newaxis])  # 当前状态下所有动作的Q值\n",
        "            q_value = q_values[0][action]  # 当前动作的Q值\n",
        "\n",
        "            # 计算目标Q值\n",
        "            if done:\n",
        "                target_q_value = reward\n",
        "            else:\n",
        "                next_q_values = model(next_state[np.newaxis])\n",
        "                target_q_value = reward + gamma * np.max(next_q_values)\n",
        "\n",
        "            # 计算损失\n",
        "            loss = tf.square(target_q_value - q_value)\n",
        "\n",
        "        # 计算梯度并更新网络\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# 训练过程\n",
        "num_episodes = 10\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(200):\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "\n",
        "        store_experience(state, action, reward, next_state, done)\n",
        "        train_q_network()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试模型\n",
        "total_rewards = 0\n",
        "for _ in range(1):  # 运行1个测试回合\n",
        "    state = env.reset()\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "    for t in range(200):\n",
        "        action = select_action(state)  # 使用训练好的模型选择动作\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = np.array(next_state, dtype=np.float32)\n",
        "        total_rewards += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "average_reward = total_rewards / 10\n",
        "print(f\"Average reward over 10 test episodes: {average_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS5SI3bRUmzH",
        "outputId": "0e906200-8e6e-4a2b-a2d2-977e613c7882"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward over 10 test episodes: 9.4\n"
          ]
        }
      ]
    }
  ]
}